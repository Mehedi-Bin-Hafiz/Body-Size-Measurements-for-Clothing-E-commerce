{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab263213",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mehedi\\PycharmProjects\\TorchEnvironment\\venvTorch\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34cb5797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "focal lenght 6614.134275618373\n",
      "face width function 342\n",
      "face width 342\n",
      "Distance: 273.65 CM\n"
     ]
    }
   ],
   "source": [
    "# install opencv \"pip install opencv-python\" \n",
    "import cv2 \n",
    "\n",
    "# distance from camera to object(face) measured \n",
    "# centimeter \n",
    "Known_distance = 175 \n",
    "  \n",
    "# width of face in the real world or Object Plane \n",
    "# centimeter \n",
    "Known_width = 14.15 #14.15 for men 13.51 for women\n",
    "\n",
    "# Colors \n",
    "GREEN = (0, 255, 0) \n",
    "RED = (0, 0, 255) \n",
    "WHITE = (255, 255, 255) \n",
    "BLACK = (0, 0, 0) \n",
    "\n",
    "image_path = \"font profile.jpg\"\n",
    "\n",
    "# defining the fonts \n",
    "fonts = cv2.FONT_HERSHEY_COMPLEX \n",
    "\n",
    "# face detector object \n",
    "face_detector = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\") \n",
    "\n",
    "# focal length finder function \n",
    "def Focal_Length_Finder(measured_distance, real_width, width_in_rf_image): \n",
    "    focal_length = (width_in_rf_image * measured_distance) / real_width \n",
    "    return focal_length #2000 is threshold\n",
    "\n",
    "# distance estimation function \n",
    "def Distance_finder(Focal_Length, real_face_width, face_width_in_frame): \n",
    "    distance = (real_face_width * Focal_Length)/face_width_in_frame \n",
    "    # return the distance \n",
    "    return distance \n",
    "\n",
    "def face_data(image): \n",
    "\n",
    "    face_width = 0 # making face width to zero \n",
    "\n",
    "    # converting color image to gray scale image \n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) \n",
    "\n",
    "    # detecting face in the image \n",
    "    faces = face_detector.detectMultiScale(gray_image, 1.3, 5) \n",
    "\n",
    "    # looping through the faces detect in the image \n",
    "    # getting coordinates x, y , width and height \n",
    "    for (x, y, h, w) in faces: \n",
    "\n",
    "        # draw the rectangle on the face \n",
    "        cv2.rectangle(image, (x, y), (x+w, y+h), GREEN, 2) \n",
    "        # getting face width in the pixels \n",
    "        face_width = w \n",
    "\n",
    "    # return the face width in pixel \n",
    "    print('face width function',face_width) \n",
    "    return face_width \n",
    "\n",
    "\n",
    "# find the face width(pixels) in the real word human image\n",
    "ref_image_face_width = 534.80 #534.80 for male 510.61 for female\n",
    "\n",
    "# get the focal by calling \"Focal_Length_Finder\" \n",
    "# face width in reference(pixels), \n",
    "# Known_distance(centimeters), \n",
    "# known_width(centimeters) \n",
    "Focal_length_found = Focal_Length_Finder(Known_distance, Known_width, ref_image_face_width) \n",
    "\n",
    "print(\"focal lenght\", Focal_length_found) \n",
    "\n",
    "# # show the reference image \n",
    "# cv2.imshow(\"ref_image\", ref_image) \n",
    "\n",
    "person_image = cv2.imread(image_path)\n",
    "# calling face_data function to find \n",
    "# the width of face(pixels) in the frame \n",
    "face_width_in_frame = face_data(person_image) \n",
    "print(\"face width\", face_width_in_frame)\n",
    "# check if the face is zero then not \n",
    "# find the distance \n",
    "if face_width_in_frame != 0: \n",
    "    # finding the distance by calling function \n",
    "    # Distance finder function need \n",
    "    # these arguments the Focal_Length, \n",
    "    # Known_width(centimeters), \n",
    "    # and Known_distance(centimeters) \n",
    "    Distance = Distance_finder(Focal_length_found, Known_width, face_width_in_frame) \n",
    "    print(f\"Distance: {round(Distance,2)} CM\")\n",
    "    # draw line as background of text \n",
    "    cv2.line(person_image, (30, 30), (230, 30), RED, 32) \n",
    "    cv2.line(person_image, (30, 30), (230, 30), BLACK, 28) \n",
    "    # Drawing Text on the screen \n",
    "    cv2.putText(person_image, f\"Distance: {round(Distance,2)} CM\", (30, 35), fonts, 0.6, GREEN, 2) \n",
    "# show the frame on the screen \n",
    "cv2.imwrite('distance measured.jpg', person_image)\n",
    "cv2.waitKey(0)  # Wait for a key press to close the window\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "880a79cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05111fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load a pretrained YOLOv8n model\n",
    "model = YOLO('yolov8m-pose.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2699ced4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\Mehedi\\Jupyter Notebook Project\\TMTSoftProject\\Cloth size estimation\\face distance measurement\\font profile.jpg: 640x384 1 person, 103.9ms\n",
      "Speed: 22.0ms preprocess, 103.9ms inference, 119.8ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns\\pose\\predict2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "results = model(source=image_path, show=False, conf=0.70, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374f5f8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46cfce68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(954.0, 3480.0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bounding_box = results[0].boxes.xyxy[0]  # Detections for the first image in batch \n",
    "x1, y1, x2, y2 = bounding_box.tolist() \n",
    "width = x2 - x1\n",
    "height = y2 - y1\n",
    "width, height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5cdf372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 996., 1016., 1950., 4496.], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bounding_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "048ffe45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3480.0, 273.6549707602339, 6614.134275618373)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "height, Distance, Focal_length_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdaf93dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53.342803323939734, 194.58381086720155)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert dimensions from pixels to centimeters\n",
    "actual_width_cm = (width * Distance) / (Focal_length_found-1720)\n",
    "actual_height_cm = (height * Distance) / (Focal_length_found-1720)\n",
    "actual_width_cm, actual_height_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c1ffbbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\Mehedi\\Jupyter Notebook Project\\TMTSoftProject\\Cloth size estimation\\face distance measurement\\side profle.jpg: 640x384 1 person, 238.6ms\n",
      "Speed: 9.7ms preprocess, 238.6ms inference, 8.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns\\pose\\predict2\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13.439905849866273"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## side measurements\n",
    "side_image = \"side profle.jpg\"\n",
    "side_results = model(source=side_image, show=False, conf=0.70, save=True)\n",
    "side_bounding_box = side_results[0].boxes.xyxy[0]  # Detections for the first image in batch \n",
    "x1, y1, x2, y2 = side_bounding_box.tolist() \n",
    "side_width = x2 - x1 \n",
    "actual_side_width_cm = ((side_width * Distance) / (Focal_length_found-1720))/2.75\n",
    "actual_side_width_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0150c400",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_values = results[0].keypoints.xy[0]\n",
    "# Extracting specific keypoints based on the request\n",
    "specific_keypoints_tensors = {\n",
    "    \"Left Shoulder\": tensor_values[5],\n",
    "    \"Right Shoulder\": tensor_values[6],\n",
    "    \"Left Elbow\": tensor_values[7],\n",
    "    \"Right Elbow\": tensor_values[8],\n",
    "    \"Left Wrist\": tensor_values[9],\n",
    "    \"Right Wrist\": tensor_values[10],\n",
    "    \"Left Hip\": tensor_values[11],\n",
    "    \"Right Hip\": tensor_values[12]\n",
    "}\n",
    "hip_adjustment = torch.norm(specific_keypoints_tensors['Left Hip'] - specific_keypoints_tensors['Right Hip']).item()\n",
    "hip_adjustment = hip_adjustment+(hip_adjustment/2)\n",
    "shoulder_adjustment = torch.norm(specific_keypoints_tensors['Left Shoulder'] - specific_keypoints_tensors['Right Shoulder']).item()\n",
    "shoulder_adjustment = shoulder_adjustment+(shoulder_adjustment/3)\n",
    "## wrist calculation.\n",
    "shoulder_to_neck =  shoulder_adjustment/2\n",
    "shouldertoelbow = torch.norm(specific_keypoints_tensors['Left Shoulder'] - specific_keypoints_tensors['Left Elbow']).item()\n",
    "elbowtowrist = torch.norm(specific_keypoints_tensors['Left Elbow'] - specific_keypoints_tensors['Left Wrist']).item()\n",
    "necktowirst = shoulder_to_neck+shouldertoelbow+elbowtowrist\n",
    "# Calculate distances using Pythagorean theorem\n",
    "pixel_distances = {\n",
    "    'chest':shoulder_adjustment ,\n",
    "    'waist': hip_adjustment,\n",
    "    'sleeve': necktowirst,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b9bdb0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chest': 109.58997158474216,\n",
       " 'waist': 90.02491230391912,\n",
       " 'sleeve': 76.28472562779463,\n",
       " 'neck': 42.82510060418658,\n",
       " 'height': 100}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert distances from pixels to cm\n",
    "measurements_cm = {key: (value * Distance) / (Focal_length_found-1720) for key, value in pixel_distances.items()}\n",
    "measurements_cm['neck'] = 2*(measurements_cm['waist']-10.16)\n",
    "measurements_cm['chest'] = 2*(measurements_cm['chest']+actual_side_width_cm)\n",
    "measurements_cm['waist'] = 2*(measurements_cm['waist']+actual_side_width_cm)\n",
    "measurements_cm['height'] = 100\n",
    "measurements_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2898e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_size(height, chest, sleeve, neck):\n",
    "    if height < 170:\n",
    "        if chest < 90:\n",
    "            return 'XS'\n",
    "        elif 90 <= chest < 96:\n",
    "            return 'S'\n",
    "        elif 96 <= chest < 102:\n",
    "            return 'M'\n",
    "        elif 102 <= chest < 108:\n",
    "            return 'L'\n",
    "        else:\n",
    "            return 'XL'\n",
    "    elif 170 <= height < 180:\n",
    "        if 90 <= chest < 96:\n",
    "            return 'S'\n",
    "        elif 96 <= chest < 102:\n",
    "            return 'M'\n",
    "        elif 102 <= chest < 108:\n",
    "            return 'L'\n",
    "        elif 108 <= chest < 114:\n",
    "            return 'XL'\n",
    "        else:\n",
    "            return 'XXL'\n",
    "    else:\n",
    "        if 96 <= chest < 102:\n",
    "            return 'M'\n",
    "        elif 102 <= chest < 108:\n",
    "            return 'L'\n",
    "        elif 108 <= chest < 114:\n",
    "            return 'XL'\n",
    "        elif 114 <= chest < 120:\n",
    "            return 'XXL'\n",
    "        else:\n",
    "            return 'XXXL'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4959f15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-shirt size: XL\n"
     ]
    }
   ],
   "source": [
    "# Calculate T-shirt size\n",
    "tshirt_size = calculate_size(measurements_cm['height'], measurements_cm['chest'], measurements_cm['sleeve'], measurements_cm['neck'])\n",
    "\n",
    "print(\"T-shirt size:\", tshirt_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19df7d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb00eda4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df4a8bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chest': 43.14565810422919,\n",
       " 'waist': 35.442878859810676,\n",
       " 'sleeve': 30.033356546375835,\n",
       " 'neck': 16.860275828419912,\n",
       " 'height': 39.370078740157474}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Conversion factor from cm to inches\n",
    "cm_to_inch = 1 / 2.54\n",
    "\n",
    "# Converting all measurements to inches\n",
    "measurements_inch = {key: value * cm_to_inch for key, value in measurements_cm.items()}\n",
    "measurements_inch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954f57a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvTorch",
   "language": "python",
   "name": "venvtorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
